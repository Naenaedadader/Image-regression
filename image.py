# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Md0Uxrj9zMGkd8w024m0p0JGdfO1Fkrj
"""

!pip install fastbook

from fastai.vision.all import*
from fastbook import*

"""Assemble the Data
using Bwi kinect head ppose dataset
"""

path = untar_data(URLs.BIWI_HEAD_POSE)

Path.BASE_PATH = path

path.ls().sorted()

(path/'01').ls().sorted()

img_files = get_image_files(path)
def img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')
img2pose(img_files[0])

im = PILImage.create(img_files[0])
im.shape

im.to_thumb(160)

cal = np.genfromtxt(path/'01'/'rgb.cal',skip_footer=6)
cal

def get_ctr(f):
  ctr = np.genfromtxt(img2pose(f), skip_header=3)
  c1 = ctr[0]*cal[0][0]/ctr[2]+cal[0][2]
  c2 = ctr[1]*cal[1][1]/ctr[2]+cal[1][2]
  return tensor([c1,c2])

get_ctr(img_files[0])

doc(Normalize.from_stats(*imagenet_stats))

biwi = DataBlock(
    blocks=(ImageBlock,PointBlock),
    get_items = get_image_files,
    get_y=get_ctr,
    splitter = FuncSplitter(lambda o: o.parent.name=='13'),
    batch_tfms=[*aug_transforms(size=(240,320)),
          Normalize.from_stats(*imagenet_stats)      ]
)

dls = biwi.dataloaders(path)
dls.show_batch(max_n=9,figsize=(8,6))

xb,yb = dls.one_batch()
xb.shape,yb.shape

yb[0]

"""Training a Model"""

learn = cnn_learner(dls,resnet18,y_range=(-1,1))

def sigmoid_range(x,lo,hi): return torch.sigmoid(x)*(hi-lo)+lo

plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4)

dls.loss_func

learn.lr_find()

learn.summary()

lr = 1e-2
learn.fine_tune(3,lr)

math.sqrt(0.0001)

learn.show_results(ds_idx=1, nrows=3, figsize=(6,8))



"""Using other picture to show the predict"""

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

from IPython.display import Image
try:
  filename = take_photo()
  print('Saved to {}'.format(filename))
  
  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

img_ts = PILImage.create('/content/photo.jpg')

img_ts

import cv2
from google.colab.patches import cv2_imshow

img_np = cv2.imread("/content/photo.jpg", cv2.IMREAD_COLOR)

img = cv2.resize(img_np, (320, 240))
a = learn.predict(img)
a

def pred_img(img, point_rad):
  img = cv2.resize(img, (320,240))
  point_cord, _, _ = learn.predict(img)

  new_img = cv2.circle(img, tuple((np.round(point_cord.tolist()[0])).astype(int)), point_rad, (0, 0, 255), -1)

  return(new_img)

imgreg_pred = pred_img(img_np, point_rad = 4)

cv2_imshow(imgreg_pred)